{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "tensor([ 0, 61,  1, 61,  2, 61,  0, 61,  3], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from v2.usta_model import UstaModel\n",
    "from v2.usta_tokenizer import UstaTokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = \"mps\"\n",
    "  \n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "u_tokenizer = UstaTokenizer(\"v1/tokenizer.json\")\n",
    "\n",
    "prompts = [\n",
    "  \"the capital of the united\",\n",
    "  \"madrid is in\",\n",
    "  \"the capital of france is\",\n",
    "  \"the capital of germany is\"\n",
    "]\n",
    "\n",
    "tokens = u_tokenizer.encode(prompts[0])\n",
    "tokens = tokens.to(device)\n",
    "print(tokens)\n",
    "batch_tokens = u_tokenizer.encode_batch(prompts, 32)\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "batch_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 32\n",
    "torch.manual_seed(1)\n",
    "u_model = UstaModel(\n",
    "  vocab_size=len(u_tokenizer.vocab),\n",
    "  embedding_dim=12,\n",
    "  num_heads=4,\n",
    "  context_length=context_length,\n",
    "  num_layers=8,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "out = u_model(batch_tokens)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 61,  1, 61,  2, 61,  0, 61,  3]], device='mps:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 61, 1, 61, 2, 61, 0, 61, 3, 17, 49]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_model.generate(tokens, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(u_model.state_dict(), \"u_model.pth\")\n",
    "\n",
    "# load model\n",
    "u_model.load_state_dict(torch.load(\"u_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/z7/wrd0w0hn7pvb9g97kmdn17640000gn/T/pip-req-build-3az1z0u8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /private/var/folders/z7/wrd0w0hn7pvb9g97kmdn17640000gn/T/pip-req-build-3az1z0u8\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")\n",
    "model = AutoModel.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VJEPA2Model(\n",
       "  (encoder): VJEPA2Encoder(\n",
       "    (embeddings): VJEPA2Embeddings(\n",
       "      (patch_embeddings): VJEPA2PatchEmbeddings3D(\n",
       "        (proj): Conv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x VJEPA2Layer(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): VJEPA2RopeAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): VJEPA2MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (predictor): VJEPA2Predictor(\n",
       "    (embeddings): VJEPA2PredictorEmbeddings(\n",
       "      (predictor_embeddings): Linear(in_features=1024, out_features=384, bias=True)\n",
       "    )\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x VJEPA2Layer(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): VJEPA2RopeAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): VJEPA2MLP(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (proj): Linear(in_features=384, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<class 'transformers.models.vjepa2.configuration_vjepa2.VJEPA2Config'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/vjepa2-vitl-fpc64-256\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1037\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1035\u001b[39m model_type = config_class_to_model_type(\u001b[38;5;28mtype\u001b[39m(config).\u001b[34m__name__\u001b[39m)\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     tokenizer_class_py, tokenizer_class_fast = \u001b[43mTOKENIZER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:811\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    809\u001b[39m         model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[mtype]\n\u001b[32m    810\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._load_attr_from_module(mtype, model_name)\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: <class 'transformers.models.vjepa2.configuration_vjepa2.VJEPA2Config'>"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/vjepa2-vitl-fpc64-256\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
