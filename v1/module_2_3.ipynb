{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 61,  1, 61,  2, 61,  3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from usta_model import UstaModel\n",
    "from usta_tokenizer import UstaTokenizer\n",
    "\n",
    "u_tokenizer = UstaTokenizer(\"tokenizer.json\")\n",
    "\n",
    "prompt = \"the capital of united\"\n",
    "\n",
    "tokens = u_tokenizer.encode(prompt)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "u_model = UstaModel(vocab_size=len(u_tokenizer.vocab), embedding_dim=4, num_heads=2, context_length=32)\n",
    "\n",
    "sentence_meanings_with_atention_context = u_model(tokens)\n",
    "sentence_meanings_with_atention_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1370, -0.3226,  0.1265,  0.1362],\n",
       "        [-0.4744, -0.2934, -0.0552,  0.0922],\n",
       "        [-0.5055, -0.2895, -0.0209,  0.1401],\n",
       "        [-0.4333, -0.2986, -0.0499,  0.0810],\n",
       "        [-0.2547, -0.2411,  0.1826,  0.2178],\n",
       "        [-0.1604, -0.3156,  0.1119,  0.1300],\n",
       "        [-0.4393, -0.2969, -0.0482,  0.0849]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model(tokens)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UstaModel(\n",
       "  (embedding): Embedding(64, 4)\n",
       "  (pos_embedding): Embedding(32, 4)\n",
       "  (self_attention): UstaMultiHeadAttention(\n",
       "    (multi_head_attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (norm): UstaLayerNorm()\n",
       "  (mlp): UstaMLP(\n",
       "    (gate_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (up_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (down_proj): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (gelu): GELU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4567, -1.4225,  0.9144,  0.9648],\n",
       "        [-1.3412, -0.5089,  0.5860,  1.2641],\n",
       "        [-1.3592, -0.4867,  0.5978,  1.2481],\n",
       "        [-1.2778, -0.6107,  0.6203,  1.2682],\n",
       "        [-1.0284, -0.9678,  0.9197,  1.0766],\n",
       "        [-0.5423, -1.3689,  0.9074,  1.0038],\n",
       "        [-1.2888, -0.5948,  0.6175,  1.2661]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from usta_layer_norm import UstaLayerNorm\n",
    "\n",
    "norm_layer = UstaLayerNorm(4)\n",
    "norm_layer(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/.pyenv/versions/3.13.3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "q_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "q_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transformer-architecture](https://deeprevision.github.io/posts/001-transformer/transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gelu](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*O5E-huBuY1UTHMmM--rhLQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8412, 1.9546, 2.9964],\n",
       "        [3.9999, 5.0000, 6.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * (\n",
    "      1 + torch.tanh(\n",
    "          torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "gelu = GELU()\n",
    "\n",
    "example_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "gelu(example_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8412, 1.9546, 2.9964],\n",
       "        [3.9999, 5.0000, 6.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch funtions \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "F.gelu(example_tensor, approximate=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class UstaMLP(nn.Module):\n",
    "  def __init__(self, embedding_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.gate_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "    self.up_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "    self.down_proj = nn.Linear(hidden_dim, embedding_dim)\n",
    "    self.gelu = GELU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" gate = self.gate_proj(x)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        up = self.up_proj(x)\n",
    "        fuse = gate * up\n",
    "        outputs = self.down_proj(fuse) \"\"\"\n",
    "    gate = self.gate_proj(x)\n",
    "    gate = self.gelu(gate)\n",
    "    up = self.up_proj(x)\n",
    "    fuse = gate * up\n",
    "    outputs = self.down_proj(fuse)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6040, -0.9759,  1.6115,  1.9684],\n",
       "        [-2.6040, -0.9759,  1.6115,  1.9684]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from usta_multi_head_attention import UstaMultiHeadAttention\n",
    "from usta_layer_norm import UstaLayerNorm\n",
    "from usta_mlp import UstaMLP\n",
    "\n",
    "class UstaDecoderBlock(nn.Module):\n",
    "  def __init__(self, embedding_dim, num_heads, context_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = UstaMultiHeadAttention(embedding_dim, embedding_dim, context_length, num_heads, dropout_rate=0.5)\n",
    "    self.norm1 = UstaLayerNorm(embedding_dim)\n",
    "    self.mlp = UstaMLP(embedding_dim, embedding_dim)\n",
    "    self.norm2 = UstaLayerNorm(embedding_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    res = self.norm1(x)\n",
    "\n",
    "    x = self.self_attention(x)\n",
    "    x = self.norm1(x)\n",
    "\n",
    "    x = x + res\n",
    "\n",
    "    res = self.norm2(x)\n",
    "    x = self.mlp(x)\n",
    "    x = self.norm2(x)\n",
    "\n",
    "    x = x + res\n",
    "\n",
    "    return x\n",
    "\n",
    "example_tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)\n",
    "decoder_block = UstaDecoderBlock(embedding_dim=4, num_heads=2, context_length=3)\n",
    "decoder_block(example_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
