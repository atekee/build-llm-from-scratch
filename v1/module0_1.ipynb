{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch tiktoken datasets matplotlib -Uq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://miro.medium.com/v2/resize:fit:850/1*nhwzi5yDdkI70uhx2KH1Dw.png](https://miro.medium.com/v2/resize:fit:850/1*nhwzi5yDdkI70uhx2KH1Dw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"the cat chased the dog\"\n",
    "text2 = \"the dog chased the cat\"\n",
    "\n",
    "text = \"the capital of france is\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tokenization\n",
    "def tokenize(text):\n",
    "  return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'chased', 'the', 'dog']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 9, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenizer.json\", \"r\") as f:\n",
    "  vocab = json.load(f)\n",
    "\n",
    "def tokenize2(text):\n",
    "  parts = text.split()\n",
    "  ids = []\n",
    "  for part in parts:\n",
    "    if part in vocab:\n",
    "      value = vocab[part]\n",
    "    else:\n",
    "      value = vocab[\"<unk>\"]\n",
    "    ids.append(value)\n",
    "  return ids\n",
    "\n",
    "token_ids1 = tokenize2(text1)\n",
    "token_ids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize2(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2502.07057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'The',\n",
       " 1: 'cat',\n",
       " 2: 'dog',\n",
       " 3: 'chased',\n",
       " 4: 'capital',\n",
       " 5: 'of',\n",
       " 6: 'France',\n",
       " 7: 'is',\n",
       " 8: '<unk>',\n",
       " 9: 'the'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab = {id: part for part, id in vocab.items()}\n",
    "reverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat chased the dog'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detokenize(ids):\n",
    "  text = \"\"\n",
    "  for id in ids:\n",
    "    part = reverse_vocab[id]\n",
    "    text += part + \" \"\n",
    "  text = text.strip()\n",
    "  return text\n",
    "  \n",
    "detokenize(token_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[464, 3797, 26172, 262, 3290]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "gpt2_ids = enc.encode(text1)\n",
    "gpt2_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The cat chased the dog', '!\"$*#')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(gpt2_ids), enc.decode(token_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200019"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[976, 9059, 135896, 290, 6446]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o200_ids = enc.encode(text1)\n",
    "o200_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-27b-it\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 818, 5866, 83755, 506, 4799]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_ids = processor.tokenizer.encode(text1)\n",
    "gemma_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<bos>The cat chased the dog', 'The cat chased the dog')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(gemma_ids), processor.tokenizer.decode(gemma_ids)[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenizer_gemma.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(processor.tokenizer.get_vocab(), f, ensure_ascii=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 58], 'states')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"tokenizer.json\")\n",
    "\n",
    "tokenizer.encode(\"states\"), tokenizer.decode([4, 58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the letter capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany. rome is in italy, \\n\\nmadrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united states is not berlin. \\nalthough these places are often mentioned together, although these capitals are often mentioned together, although these are often mentioned together, \\neach country has its own capital, and each country has its own city, and each capital has its own identity, and each capital has its own history. washington \\nis the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, and berlin is known for art and \\nhistory, and rome is known for art and history, and madrid is known for culture and history, and lisbon is known for culture and art. rome is rich with culture, \\nrome is rich with history, rome is rich with art, and madrid is rich with art and culture. lisbon is a unique city in portugal with a rich history, a rich culture, \\nand a rich identity. these capitals are often mentioned together, these capitals are often mentioned together in art, these capitals are often mentioned together \\nin culture, these capitals are often mentioned together in history. the united states is not in europe, the united states is not in any european place, and \\nwashington is not in any european city. each european country is made of important capitals, and each european capital is made of art, history, and culture. \\nthe capital of the united states is washington, the capital of the united kingdom is london, the capital of france is paris, the capital of germany is berlin, \\nthe capital of italy is rome, the capital of spain is madrid, and the capital of portugal is lisbon. while these capitals are in europe, while these capitals are \\nin europe, washington is in the united states. these capitals remain important, these remain important, these places remain important in the world. the \\ncapital of each country is its own, the capital of each country is its identity, the capital of each country is its culture. europe is made of many, \\neurope is made of many capitals, europe is made of many important places. each place is rich with culture, each place is rich with history, and each capital is \\n\\nrich with identity. the world is made of capitals, the world is made of, the world is made of places, and the capital of the united states is washington, \\nnot any european city, not paris, not london, not berlin. the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany.\\nrome is in italy, madrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united \\nstates is not berlin. although these places are often mentioned together, each country has its own capital, and each capital has its own identity. \\nwashington is the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, while berlin is \\nfamous for its culture and history. rome is rich with history, and madrid is known for its art and culture. lisbon is a unique city in portugal \\nwith a rich history. these capitals are often mentioned together, although each place with its own culture. the united states is not in europe, \\nand washington is not in any european country. these european capitals are made of history, culture, and identity. each country in europe has a capital, \\nand each capital is known for important. london, paris, berlin, rome, madrid, and lisbon remain important places in the world. while these capitals\\nare in europe, washington is in the united states. although these places are not in the country, they are often mentioned together in art, culture, \\nand history. the capital of each country is its own. europe is made of many capitals, and each has a capital with a unique history. \\nthe world is of important places, and the capital of the united states is washington, not any european city.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 7,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 8,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 11,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 12,\n",
       " 59,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 15,\n",
       " 60,\n",
       " 61,\n",
       " 16,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 17,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 18,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 19,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 20,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 11,\n",
       " 59,\n",
       " 61,\n",
       " 22,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 22,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 22,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 1,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 37,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 34,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 43,\n",
       " 59,\n",
       " 61,\n",
       " 21,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 7,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 20,\n",
       " 59,\n",
       " 61,\n",
       " 9,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 54,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 11,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 16,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 44,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 18,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 44,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 53,\n",
       " 59,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 16,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 44,\n",
       " 59,\n",
       " 61,\n",
       " 18,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 48,\n",
       " 61,\n",
       " 37,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 19,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 34,\n",
       " 59,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 43,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 45,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 35,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 24,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 21,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 35,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 37,\n",
       " 59,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 39,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 44,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 21,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 20,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 7,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 8,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 12,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 11,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 15,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 13,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 17,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 16,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 19,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 18,\n",
       " 59,\n",
       " 61,\n",
       " 50,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 45,\n",
       " 60,\n",
       " 61,\n",
       " 50,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 45,\n",
       " 60,\n",
       " 61,\n",
       " 21,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 59,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 38,\n",
       " 61,\n",
       " 39,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 38,\n",
       " 61,\n",
       " 39,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 61,\n",
       " 38,\n",
       " 61,\n",
       " 39,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 34,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 44,\n",
       " 59,\n",
       " 61,\n",
       " 45,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 47,\n",
       " 60,\n",
       " 61,\n",
       " 45,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 47,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 45,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 47,\n",
       " 61,\n",
       " 39,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 59,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 24,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 24,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 34,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 21,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 35,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 37,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 7,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 11,\n",
       " 59,\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentencepiece -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 64\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 27 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=4081\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=25\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 27 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 27\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 100\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=125 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=20 all=207 active=182 piece=nt\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "  input=\"text.txt\",\n",
    "  model_prefix=\"spm_tokenizer\",\n",
    "  vocab_size=64,\n",
    "  model_type=\"bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([39, 0, 48, 42, 7, 41, 40, 7, 48, 41, 46, 30, 9, 39, 51, 45, 60],\n",
       " ['▁',\n",
       "  'T',\n",
       "  'h',\n",
       "  'e',\n",
       "  '▁c',\n",
       "  'a',\n",
       "  't',\n",
       "  '▁c',\n",
       "  'h',\n",
       "  'a',\n",
       "  's',\n",
       "  'ed',\n",
       "  '▁the',\n",
       "  '▁',\n",
       "  'd',\n",
       "  'o',\n",
       "  'g'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_tokenizer = spm.SentencePieceProcessor(model_file=\"spm_tokenizer.model\")\n",
    "\n",
    "spm_ids = spm_tokenizer.Encode(text1)\n",
    "spm_tokens = spm_tokenizer.Encode(text1, out_type=str)\n",
    "spm_ids, spm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tokenizer -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = BpeTrainer(vocab_size=64, special_tokens=[\"<unk>\"])\n",
    "\n",
    "hf_tokenizer.train([\"text.txt\"], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, [28, 5, 3, 21, 5, 10, 49, 44, 28, 6, 16, 9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.get_vocab_size(), hf_tokenizer.encode(text1).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.save(\"hf_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 5, 3, 21, 5, 10, 49, 44, 28, 6, 16, 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"hf_tokenizer.json\")\n",
    "\n",
    "fast_tokenizer.encode(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cli login to huggingface\n",
    "\n",
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/hf_tokenizer/commit/ae14106a4f8cf9e12903aacfa7f4a0f47bb82de3', commit_message='Upload tokenizer', commit_description='', oid='ae14106a4f8cf9e12903aacfa7f4a0f47bb82de3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/hf_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/hf_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.push_to_hub(\"alibayram/hf_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
