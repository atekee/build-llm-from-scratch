{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "tensor([ 0, 61,  1, 61,  2, 61,  0, 61,  3], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from usta_model import UstaModel\n",
    "from usta_tokenizer import UstaTokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = \"mps\"\n",
    "  \n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "u_tokenizer = UstaTokenizer(\"tokenizer.json\")\n",
    "\n",
    "prompts = [\n",
    "  \"the capital of the united\",\n",
    "  \"madrid is in\",\n",
    "  \"the capital of france is\",\n",
    "  \"the capital of germany is\"\n",
    "]\n",
    "\n",
    "tokens = u_tokenizer.encode(prompts[0])\n",
    "tokens = tokens.to(device)\n",
    "print(tokens)\n",
    "batch_tokens = u_tokenizer.encode_batch(prompts, 32)\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "batch_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "context_length = 32\n",
    "torch.manual_seed(1)\n",
    "u_model = UstaModel(\n",
    "  vocab_size=len(u_tokenizer.vocab),\n",
    "  embedding_dim=12,\n",
    "  num_heads=4,\n",
    "  context_length=context_length,\n",
    "  num_layers=8,\n",
    "  device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model(batch_tokens)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.flatten(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the capital of the unitedspainworldspain'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model.generate(tokens, 3)\n",
    "u_tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099,\n",
       " 'the capital of the united states is not london. the capital of france is paris, and berlin is the ca')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "len(text), text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1593, torch.Tensor)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = u_tokenizer.encode(text)\n",
    "len(token_ids), type(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_dataset import create_data_loader\n",
    "\n",
    "stride = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader = create_data_loader(token_ids.tolist(), context_length, stride, 16, False)\n",
    "\n",
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11776\n",
      "UstaModel(\n",
      "  (embedding): UstaEmbedding(\n",
      "    (embedding): Embedding(64, 12)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (1): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (2): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (3): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (4): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (5): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (6): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (7): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=12, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model parameters count\n",
    "parameters_count = sum(p.numel() for p in u_model.parameters())\n",
    "print(parameters_count)\n",
    "\n",
    "# model architecture\n",
    "print(u_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(u_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i, (X, Y) in enumerate(train_data_loader):\n",
    "  print(X.shape, Y.shape, Y.flatten().shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 3.9492006301879883 average loss: 4.146813604566786\n",
      "Epoch 2 loss: 3.680936574935913 average loss: 3.7348970307244196\n",
      "Epoch 3 loss: 3.499462127685547 average loss: 3.494166135787964\n",
      "Epoch 4 loss: 3.271186113357544 average loss: 3.299847576353285\n",
      "Epoch 5 loss: 3.110752820968628 average loss: 3.1378757423824735\n",
      "Epoch 6 loss: 2.9369232654571533 average loss: 3.003199232949151\n",
      "Epoch 7 loss: 2.8485686779022217 average loss: 2.914712217119005\n",
      "Epoch 8 loss: 2.7624175548553467 average loss: 2.8482564290364585\n",
      "Epoch 9 loss: 2.732391119003296 average loss: 2.807366715537177\n",
      "Epoch 10 loss: 2.704106569290161 average loss: 2.7695599926842585\n",
      "Epoch 11 loss: 2.656946897506714 average loss: 2.725328180525038\n",
      "Epoch 12 loss: 2.5418789386749268 average loss: 2.636570241716173\n",
      "Epoch 13 loss: 2.4706923961639404 average loss: 2.546763022740682\n",
      "Epoch 14 loss: 2.359086036682129 average loss: 2.4571994145711265\n",
      "Epoch 15 loss: 2.327404737472534 average loss: 2.398976299497816\n",
      "Epoch 16 loss: 2.221524238586426 average loss: 2.3395028114318848\n",
      "Epoch 17 loss: 2.1757102012634277 average loss: 2.2979851033952503\n",
      "Epoch 18 loss: 2.169156789779663 average loss: 2.266634358300103\n",
      "Epoch 19 loss: 2.1589527130126953 average loss: 2.244144837061564\n",
      "Epoch 20 loss: 2.1059577465057373 average loss: 2.2158791224161782\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "\n",
    "for epoch in range(epoch):\n",
    "  total_loss = 0.\n",
    "  for i, (X, Y) in enumerate(train_data_loader):\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    \n",
    "    pred = u_model(X)\n",
    "    loss = loss_fn(pred.flatten(0, 1), Y.flatten())\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  average_loss = total_loss / len(train_data_loader)\n",
    "  print(f\"Epoch {epoch + 1} loss: {loss.item()} average loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7481, device='mps:0', grad_fn=<MaxBackward0>),\n",
       " tensor(61, device='mps:0'),\n",
       " tensor([7.2462e-03, 1.1836e-02, 3.8140e-03, 3.7636e-03, 1.1475e-03, 8.4088e-03,\n",
       "         1.5379e-03, 1.9477e-03, 4.0389e-04, 7.3157e-04, 5.1895e-03, 1.0106e-03,\n",
       "         1.7649e-03, 1.4185e-03, 2.8122e-03, 5.6158e-04, 9.2819e-04, 1.9205e-03,\n",
       "         7.1523e-04, 6.0637e-04, 7.6060e-04, 1.6119e-03, 7.4009e-04, 2.3392e-03,\n",
       "         1.8434e-03, 2.6202e-03, 2.3143e-03, 2.0668e-03, 3.8642e-03, 3.5154e-03,\n",
       "         1.7726e-03, 1.5751e-03, 1.4186e-03, 6.2729e-04, 1.3170e-03, 3.9297e-03,\n",
       "         2.1808e-03, 7.0981e-04, 4.2417e-04, 1.8735e-03, 1.8163e-03, 1.4317e-03,\n",
       "         1.6790e-03, 5.4227e-03, 2.2217e-03, 1.6779e-03, 5.6502e-03, 2.3212e-04,\n",
       "         4.1757e-04, 1.8621e-03, 2.1203e-04, 6.2056e-04, 1.4454e-03, 4.1037e-03,\n",
       "         1.5938e-04, 8.5594e-04, 1.8411e-04, 2.8391e-03, 2.6055e-02, 3.7270e-02,\n",
       "         5.9321e-02, 7.4812e-01, 7.5976e-04, 3.7013e-04], device='mps:0',\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_tokens = u_tokenizer.encode(\"the capital of the united states is\")\n",
    "new_tokens = new_tokens.tolist()\n",
    "# new_tokens.append(61)\n",
    "\n",
    "out = u_model(torch.tensor([new_tokens]).to(device))\n",
    "out = out.squeeze(0)\n",
    "probs = torch.softmax(out[-1], dim=-1)\n",
    "max_prob, max_index = torch.max(probs, dim=-1)\n",
    "max_prob, max_index, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(u_model.state_dict(), \"u_model.pth\")\n",
    "\n",
    "# load model\n",
    "u_model.load_state_dict(torch.load(\"u_model.pth\"))\n",
    "\n",
    "# generate text\n",
    "new_tokens = u_tokenizer.encode(\"the capital of the united states is london. the capital of france is\")\n",
    "new_tokens = new_tokens.detach().cpu().numpy().tolist()\n",
    "new_tokens.append(61)\n",
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UstaModel(\n",
       "  (embedding): UstaEmbedding(\n",
       "    (embedding): Embedding(64, 12)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (1): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (2): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (3): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (4): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (5): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (6): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "    (7): UstaDecoderBlock(\n",
       "      (self_attention): UstaMultiHeadAttention(\n",
       "        (multi_head_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
       "        )\n",
       "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
       "      )\n",
       "      (norm1): UstaLayerNorm()\n",
       "      (mlp): UstaMLP(\n",
       "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (gelu): GELU()\n",
       "      )\n",
       "      (norm2): UstaLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=12, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = UstaModel(64, embedding_dim=12, num_heads=4, context_length=32, num_layers=8, device=device)\n",
    "loaded_model.load_state_dict(torch.load(\"u_model.pth\"))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7614, 0.0889, 0.7531, 0.0840, 0.7518, 0.1274, 0.7077, 0.0873, 0.7415,\n",
       "         0.0781, 0.0856, 0.7514, 0.0787, 0.7031, 0.0862, 0.7157, 0.7418, 0.1250,\n",
       "         0.6935, 0.0783, 0.7121, 0.0754, 0.7380, 0.0662, 0.7589, 0.0715, 0.7227,\n",
       "         0.0984], device='mps:0', grad_fn=<MaxBackward0>),\n",
       " tensor([61,  5, 61,  5, 61, 61, 61,  5, 61,  5,  5, 61,  5, 61,  5, 61, 61, 60,\n",
       "         61,  5, 61,  5, 61,  0, 61,  5, 61,  5], device='mps:0'),\n",
       " tensor([[6.6302e-03, 9.4189e-03, 3.1651e-03,  ..., 7.6135e-01, 6.1700e-04,\n",
       "          4.8210e-04],\n",
       "         [5.0905e-02, 3.6659e-02, 4.5700e-02,  ..., 1.9057e-02, 1.0943e-02,\n",
       "          2.6898e-03],\n",
       "         [5.7731e-03, 9.8042e-03, 3.2646e-03,  ..., 7.5312e-01, 7.3495e-04,\n",
       "          5.0608e-04],\n",
       "         ...,\n",
       "         [5.8093e-02, 4.1894e-02, 4.6945e-02,  ..., 5.5131e-02, 4.8598e-03,\n",
       "          3.9507e-03],\n",
       "         [7.8542e-03, 1.4592e-02, 4.6774e-03,  ..., 7.2268e-01, 8.7576e-04,\n",
       "          4.2699e-04],\n",
       "         [6.4769e-02, 3.6684e-02, 5.1999e-02,  ..., 7.0831e-02, 8.5768e-03,\n",
       "          1.8635e-03]], device='mps:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model(torch.tensor(new_tokens).unsqueeze(0).to(device))\n",
    "\n",
    "probs = torch.softmax(out[-1], dim=-1)\n",
    "max_prob, max_index = torch.max(probs, dim=-1)\n",
    "max_prob, max_index, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 61, 5, 61, 14, 61, 5, 61]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_tokens = u_tokenizer.encode(\"madrid is in\")\n",
    "new_tokens = new_tokens.detach().cpu().numpy().tolist()\n",
    "new_tokens.append(61)\n",
    "\n",
    "u_model.generate(torch.tensor(new_tokens), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
