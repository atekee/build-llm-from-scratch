{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "tensor([ 0, 61,  1, 61,  2, 61,  0, 61,  3], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from usta_model import UstaModel\n",
    "from usta_tokenizer import UstaTokenizer\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = \"mps\"\n",
    "  \n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "u_tokenizer = UstaTokenizer(\"tokenizer.json\")\n",
    "\n",
    "prompts = [\n",
    "  \"the capital of the united\",\n",
    "  \"madrid is in\",\n",
    "  \"the capital of france is\",\n",
    "  \"the capital of germany is\"\n",
    "]\n",
    "\n",
    "tokens = u_tokenizer.encode(prompts[0])\n",
    "tokens = tokens.to(device)\n",
    "print(tokens)\n",
    "batch_tokens = u_tokenizer.encode_batch(prompts, 32)\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "batch_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "context_length = 32\n",
    "torch.manual_seed(1)\n",
    "u_model = UstaModel(\n",
    "  vocab_size=len(u_tokenizer.vocab),\n",
    "  embedding_dim=12,\n",
    "  num_heads=4,\n",
    "  context_length=context_length,\n",
    "  num_layers=8,\n",
    "  device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model(batch_tokens)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.flatten(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the capital of the unitedspainworldspain'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model.generate(tokens, 3)\n",
    "u_tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099,\n",
       " 'the capital of the united states is not london. the capital of france is paris, and berlin is the ca')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "len(text), text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1593, torch.Tensor)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = u_tokenizer.encode(text)\n",
    "len(token_ids), type(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_dataset import create_data_loader\n",
    "\n",
    "stride = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_loader = create_data_loader(token_ids.tolist(), context_length, stride, 16, False)\n",
    "\n",
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11776\n",
      "UstaModel(\n",
      "  (embedding): UstaEmbedding(\n",
      "    (embedding): Embedding(64, 12)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (1): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (2): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (3): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (4): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (5): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (6): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (7): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=12, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model parameters count\n",
    "parameters_count = sum(p.numel() for p in u_model.parameters())\n",
    "print(parameters_count)\n",
    "\n",
    "# model architecture\n",
    "print(u_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(u_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32]) torch.Size([16, 32]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i, (X, Y) in enumerate(train_data_loader):\n",
    "  print(X.shape, Y.shape, Y.flatten().shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 4.4843926429748535 average loss: 4.4794751273261175\n",
      "Epoch 2 loss: 4.446028232574463 average loss: 4.470196353064643\n",
      "Epoch 3 loss: 4.4884514808654785 average loss: 4.474286609225803\n",
      "Epoch 4 loss: 4.445725917816162 average loss: 4.481040583716498\n",
      "Epoch 5 loss: 4.473413467407227 average loss: 4.472801579369439\n",
      "Epoch 6 loss: 4.4503936767578125 average loss: 4.466412650214301\n",
      "Epoch 7 loss: 4.459188461303711 average loss: 4.47643502553304\n",
      "Epoch 8 loss: 4.482313632965088 average loss: 4.473783387078179\n",
      "Epoch 9 loss: 4.45524263381958 average loss: 4.477642642127143\n",
      "Epoch 10 loss: 4.433439254760742 average loss: 4.4754940668741865\n",
      "Epoch 11 loss: 4.50478982925415 average loss: 4.4762384626600475\n",
      "Epoch 12 loss: 4.487969875335693 average loss: 4.481786992814806\n",
      "Epoch 13 loss: 4.512622833251953 average loss: 4.478790707058376\n",
      "Epoch 14 loss: 4.490748882293701 average loss: 4.483398225572374\n",
      "Epoch 15 loss: 4.511246204376221 average loss: 4.477768792046441\n",
      "Epoch 16 loss: 4.5075836181640625 average loss: 4.4767231941223145\n",
      "Epoch 17 loss: 4.512115001678467 average loss: 4.477860503726536\n",
      "Epoch 18 loss: 4.479860782623291 average loss: 4.475730472140842\n",
      "Epoch 19 loss: 4.47295618057251 average loss: 4.483163303799099\n",
      "Epoch 20 loss: 4.4190287590026855 average loss: 4.473934226565891\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "\n",
    "for epoch in range(epoch):\n",
    "  total_loss = 0.\n",
    "  for i, (X, Y) in enumerate(train_data_loader):\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "    \n",
    "    pred = u_model(X)\n",
    "    loss = loss_fn(pred.flatten(0, 1), Y.flatten())\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "  average_loss = total_loss / len(train_data_loader)\n",
    "  print(f\"Epoch {epoch + 1} loss: {loss.item()} average loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8437, device='mps:0', grad_fn=<MaxBackward0>),\n",
       " tensor(61, device='mps:0'),\n",
       " tensor([1.7689e-04, 1.2702e-04, 8.9334e-05, 1.1294e-04, 4.8348e-05, 1.7324e-04,\n",
       "         1.1038e-04, 1.0911e-04, 3.3815e-05, 3.5535e-05, 1.1334e-04, 4.2045e-05,\n",
       "         4.1052e-05, 5.8707e-05, 1.1835e-04, 3.3369e-05, 2.4978e-05, 5.0302e-05,\n",
       "         3.2275e-05, 4.6224e-05, 4.6559e-05, 6.1137e-05, 1.7074e-05, 3.7541e-05,\n",
       "         5.5173e-05, 5.3847e-05, 2.6077e-05, 5.0324e-05, 5.6361e-05, 8.1367e-05,\n",
       "         6.9361e-05, 4.9794e-05, 3.6766e-05, 5.1926e-05, 2.1399e-05, 7.9568e-05,\n",
       "         5.1449e-05, 3.6105e-05, 4.4190e-05, 7.7000e-05, 5.5887e-05, 8.8757e-05,\n",
       "         6.3735e-05, 1.0779e-04, 6.7966e-05, 5.8942e-05, 1.4082e-04, 1.9556e-05,\n",
       "         4.9123e-05, 1.2208e-04, 1.0568e-05, 3.7774e-05, 8.1377e-05, 9.0780e-05,\n",
       "         1.7489e-05, 4.7874e-05, 1.5352e-05, 5.1402e-03, 3.3147e-02, 5.7185e-02,\n",
       "         5.7015e-02, 8.4370e-01, 8.3003e-05, 7.7342e-05], device='mps:0',\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_tokens = u_tokenizer.encode(\"the capital of the united states is\")\n",
    "new_tokens = new_tokens.tolist()\n",
    "# new_tokens.append(61)\n",
    "\n",
    "out = u_model(torch.tensor([new_tokens]).to(device))\n",
    "out = out.squeeze(0)\n",
    "probs = torch.softmax(out[-1], dim=-1)\n",
    "max_prob, max_index = torch.max(probs, dim=-1)\n",
    "max_prob, max_index, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "torch.save(u_model.state_dict(), \"u_model.pth\")\n",
    "\n",
    "# load model\n",
    "u_model.load_state_dict(torch.load(\"u_model.pth\"))\n",
    "\n",
    "# generate text\n",
    "new_tokens = u_tokenizer.encode(\"the capital of the united states is london. the capital of france is\")\n",
    "new_tokens = new_tokens.detach().cpu().numpy().tolist()\n",
    "new_tokens.append(61)\n",
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UstaModel:\n\tMissing key(s) in state_dict: \"embedding.embedding.weight\". \n\tUnexpected key(s) in state_dict: \"pos_embedding.weight\", \"embedding.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m loaded_model = UstaModel(\u001b[32m64\u001b[39m, embedding_dim=\u001b[32m12\u001b[39m, num_heads=\u001b[32m4\u001b[39m, context_length=\u001b[32m32\u001b[39m, num_layers=\u001b[32m8\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mu_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m loaded_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.3/lib/python3.13/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for UstaModel:\n\tMissing key(s) in state_dict: \"embedding.embedding.weight\". \n\tUnexpected key(s) in state_dict: \"pos_embedding.weight\", \"embedding.weight\". "
     ]
    }
   ],
   "source": [
    "loaded_model = UstaModel(64, embedding_dim=12, num_heads=4, context_length=32, num_layers=8)\n",
    "loaded_model.load_state_dict(torch.load(\"u_model.pth\"))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9950, grad_fn=<MaxBackward0>),\n",
       " tensor(9),\n",
       " tensor([9.0979e-04, 1.8543e-10, 1.5124e-08, 3.6638e-08, 1.7322e-08, 1.4591e-08,\n",
       "         1.7032e-04, 1.1479e-05, 4.5102e-10, 9.9498e-01, 4.7338e-09, 1.7963e-05,\n",
       "         3.0512e-06, 5.1489e-07, 4.5850e-07, 1.1249e-09, 8.1628e-06, 4.8592e-11,\n",
       "         1.7493e-07, 1.5918e-13, 6.1456e-11, 6.0847e-07, 1.2491e-03, 2.5757e-05,\n",
       "         3.0324e-09, 9.3538e-10, 2.9011e-10, 1.9273e-13, 2.5738e-11, 1.7907e-05,\n",
       "         2.4082e-03, 1.8547e-07, 1.4759e-05, 1.3782e-09, 7.1770e-07, 3.2794e-11,\n",
       "         7.2374e-10, 6.6117e-10, 2.7632e-11, 2.0459e-10, 3.3138e-07, 1.8605e-05,\n",
       "         2.4547e-08, 2.8324e-11, 3.2160e-07, 1.4761e-11, 3.6142e-06, 2.6393e-09,\n",
       "         1.1043e-07, 1.8352e-13, 3.5876e-05, 1.8231e-07, 1.3335e-10, 2.6382e-14,\n",
       "         3.5302e-10, 1.1375e-04, 2.5035e-07, 3.2066e-08, 5.8043e-06, 1.3569e-08,\n",
       "         1.4145e-11, 1.9435e-10, 7.8819e-12, 7.8819e-12],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = u_model(torch.tensor(new_tokens))\n",
    "\n",
    "probs = torch.softmax(out[-1], dim=-1)\n",
    "max_prob, max_index = torch.max(probs, dim=-1)\n",
    "max_prob, max_index, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m new_tokens = new_tokens.detach().cpu().numpy().tolist()\n\u001b[32m      5\u001b[39m new_tokens.append(\u001b[32m61\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mu_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/llms/llm-from-scratch/usta_model.py:35\u001b[39m, in \u001b[36mUstaModel.generate\u001b[39m\u001b[34m(self, x, max_new_tokens)\u001b[39m\n\u001b[32m     33\u001b[39m   _, max_index = torch.max(probs, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m   tokens.append(max_index)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m   x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[31mTypeError\u001b[39m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_tokens = u_tokenizer.encode(\"madrid is in\")\n",
    "new_tokens = new_tokens.detach().cpu().numpy().tolist()\n",
    "new_tokens.append(61)\n",
    "\n",
    "u_model.generate(torch.tensor(new_tokens), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
