{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 61,  1, 61,  2, 61,  0, 61,  3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from usta_model import UstaModel\n",
    "from usta_tokenizer import UstaTokenizer\n",
    "\n",
    "u_tokenizer = UstaTokenizer(\"tokenizer.json\")\n",
    "\n",
    "prompt = \"the capital of the united\"\n",
    "\n",
    "tokens = u_tokenizer.encode(prompt)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "u_model = UstaModel(vocab_size=len(u_tokenizer.vocab), embedding_dim=12, num_heads=4, context_length=context_length, num_layers=8)\n",
    "\n",
    "out = u_model(tokens)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099,\n",
       " 'the capital of the united states is not london. the capital of france is paris, and berlin is the ca')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "len(text), text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1593, torch.Tensor)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = u_tokenizer.encode(text)\n",
    "len(token_ids), type(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1593, list)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = token_ids.detach().cpu().numpy().tolist()\n",
    "len(ids), type(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 131)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_dataset import TextDataset\n",
    "\n",
    "stride = 12\n",
    "\n",
    "dataset = TextDataset(ids, context_length, stride)\n",
    "\n",
    "len(dataset.inputs), len(dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 61,  1, 61,  2, 61,  0, 61,  3, 61,  4, 58, 61,  5, 61,  6, 61,  7,\n",
       "         59, 61,  0, 61,  1, 61,  2, 61,  8, 61,  5, 61,  9, 60]),\n",
       " tensor([61,  1, 61,  2, 61,  0, 61,  3, 61,  4, 58, 61,  5, 61,  6, 61,  7, 59,\n",
       "         61,  0, 61,  1, 61,  2, 61,  8, 61,  5, 61,  9, 60, 61]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.inputs[0], dataset.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12160\n",
      "UstaModel(\n",
      "  (embedding): Embedding(64, 12)\n",
      "  (pos_embedding): Embedding(32, 12)\n",
      "  (layers): Sequential(\n",
      "    (0): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (1): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (2): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (3): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (4): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (5): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (6): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "    (7): UstaDecoderBlock(\n",
      "      (self_attention): UstaMultiHeadAttention(\n",
      "        (multi_head_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)\n",
      "        )\n",
      "        (projection): Linear(in_features=12, out_features=12, bias=True)\n",
      "      )\n",
      "      (norm1): UstaLayerNorm()\n",
      "      (mlp): UstaMLP(\n",
      "        (gate_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (up_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (down_proj): Linear(in_features=12, out_features=12, bias=True)\n",
      "        (gelu): GELU()\n",
      "      )\n",
      "      (norm2): UstaLayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=12, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model parameters count\n",
    "parameters_count = sum(p.numel() for p in u_model.parameters())\n",
    "print(parameters_count)\n",
    "\n",
    "# model architecture\n",
    "print(u_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out0 = u_model(dataset.inputs[0])\n",
    "out0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5694, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(out0, dataset.targets[0])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5694499015808105"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(u_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for input, target in dataset:\n",
    "  print(input.shape, target.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.1365489959716797 average loss: 1.0340135848249188\n",
      "Epoch 2 loss: 0.8841158747673035 average loss: 1.0448410583816412\n",
      "Epoch 3 loss: 0.9503324031829834 average loss: 1.001277290910255\n",
      "Epoch 4 loss: 0.818622350692749 average loss: 1.0407238156740901\n",
      "Epoch 5 loss: 0.9001615047454834 average loss: 1.0215074263001216\n",
      "Epoch 6 loss: 0.803684413433075 average loss: 0.9907639099441412\n",
      "Epoch 7 loss: 0.8893896341323853 average loss: 0.9944450029435049\n",
      "Epoch 8 loss: 1.0684871673583984 average loss: 1.0058494647040621\n",
      "Epoch 9 loss: 1.0432997941970825 average loss: 0.9971340703600235\n",
      "Epoch 10 loss: 0.9450849294662476 average loss: 0.9730800422093341\n",
      "Epoch 11 loss: 1.041717290878296 average loss: 0.9997373528152932\n",
      "Epoch 12 loss: 0.8566493391990662 average loss: 0.9882412748482391\n",
      "Epoch 13 loss: 0.9452464580535889 average loss: 1.0028933013668497\n",
      "Epoch 14 loss: 0.9631799459457397 average loss: 1.0004170891892818\n",
      "Epoch 15 loss: 0.778756856918335 average loss: 1.0041923882397077\n",
      "Epoch 16 loss: 0.8021661639213562 average loss: 0.9916708969887886\n",
      "Epoch 17 loss: 0.8663895726203918 average loss: 0.9698371855357221\n",
      "Epoch 18 loss: 0.9566458463668823 average loss: 0.9678512645586757\n",
      "Epoch 19 loss: 0.7730579376220703 average loss: 0.9500093696681597\n",
      "Epoch 20 loss: 1.187727451324463 average loss: 0.9641121482576123\n",
      "Epoch 21 loss: 1.0629252195358276 average loss: 0.955051107943513\n",
      "Epoch 22 loss: 1.0120729207992554 average loss: 0.9435331148045664\n",
      "Epoch 23 loss: 0.9534894824028015 average loss: 0.9582818043595962\n",
      "Epoch 24 loss: 0.8415228724479675 average loss: 0.9571318426205002\n",
      "Epoch 25 loss: 0.7268948554992676 average loss: 0.9534038157408474\n",
      "Epoch 26 loss: 0.8277277946472168 average loss: 0.9382300618040653\n",
      "Epoch 27 loss: 0.791415274143219 average loss: 0.9476219199540961\n",
      "Epoch 28 loss: 0.9327938556671143 average loss: 0.9417461404818614\n",
      "Epoch 29 loss: 0.9198659062385559 average loss: 0.93576114323303\n",
      "Epoch 30 loss: 0.9057395458221436 average loss: 0.9376612494465049\n",
      "Epoch 31 loss: 1.6275802850723267 average loss: 0.9280017432365709\n",
      "Epoch 32 loss: 0.7471188306808472 average loss: 0.9392304729869347\n",
      "Epoch 33 loss: 1.1037708520889282 average loss: 0.9267022291212591\n",
      "Epoch 34 loss: 0.8461122512817383 average loss: 0.9207181025097388\n",
      "Epoch 35 loss: 0.8282370567321777 average loss: 0.9273831302883061\n",
      "Epoch 36 loss: 0.8674374222755432 average loss: 0.8965332596811629\n",
      "Epoch 37 loss: 0.8537508845329285 average loss: 0.8934300851275903\n",
      "Epoch 38 loss: 0.7325681447982788 average loss: 0.9402663273210744\n",
      "Epoch 39 loss: 0.7595053911209106 average loss: 0.9031011089113833\n",
      "Epoch 40 loss: 0.8223637342453003 average loss: 0.9336327955923007\n",
      "Epoch 41 loss: 0.681715726852417 average loss: 0.9073022619913552\n",
      "Epoch 42 loss: 0.8020285964012146 average loss: 0.8995027944786858\n",
      "Epoch 43 loss: 0.9504076838493347 average loss: 0.900362282536412\n",
      "Epoch 44 loss: 0.8419917821884155 average loss: 0.9063756431787069\n",
      "Epoch 45 loss: 0.7653458118438721 average loss: 0.9060864186923923\n",
      "Epoch 46 loss: 0.8927527070045471 average loss: 0.8957882769235218\n",
      "Epoch 47 loss: 0.8833507895469666 average loss: 0.9041981412709215\n",
      "Epoch 48 loss: 0.7641078233718872 average loss: 0.9082564948169329\n",
      "Epoch 49 loss: 0.8506374955177307 average loss: 0.9075163784827894\n",
      "Epoch 50 loss: 0.8821883201599121 average loss: 0.8953073181723821\n",
      "Epoch 51 loss: 0.7711659669876099 average loss: 0.9163089487843841\n",
      "Epoch 52 loss: 0.730745255947113 average loss: 0.877031650707012\n",
      "Epoch 53 loss: 0.7344008684158325 average loss: 0.8887662623674815\n",
      "Epoch 54 loss: 0.8049401044845581 average loss: 0.9064342770867675\n",
      "Epoch 55 loss: 0.8025204539299011 average loss: 0.8736887865394126\n",
      "Epoch 56 loss: 0.7180078029632568 average loss: 0.8883568230476088\n",
      "Epoch 57 loss: 0.670279324054718 average loss: 0.8585154944248782\n",
      "Epoch 58 loss: 0.6948429346084595 average loss: 0.8796536010185271\n",
      "Epoch 59 loss: 0.6895997524261475 average loss: 0.8482518448629452\n",
      "Epoch 60 loss: 0.8698872327804565 average loss: 0.869693519959923\n",
      "Epoch 61 loss: 0.7884713411331177 average loss: 0.8467619255298876\n",
      "Epoch 62 loss: 0.732738196849823 average loss: 0.8673222813442463\n",
      "Epoch 63 loss: 0.9256976842880249 average loss: 0.8694465654041931\n",
      "Epoch 64 loss: 0.76957106590271 average loss: 0.8789155970092948\n",
      "Epoch 65 loss: 0.751261830329895 average loss: 0.8547089277332975\n",
      "Epoch 66 loss: 0.8318089246749878 average loss: 0.8669128536268045\n",
      "Epoch 67 loss: 0.7966888546943665 average loss: 0.8464613986834315\n",
      "Epoch 68 loss: 0.7915162444114685 average loss: 0.8535330397937134\n",
      "Epoch 69 loss: 0.7054879665374756 average loss: 0.8508509727379748\n",
      "Epoch 70 loss: 0.5833024382591248 average loss: 0.8666937251127403\n",
      "Epoch 71 loss: 0.7472993731498718 average loss: 0.8355383392963701\n",
      "Epoch 72 loss: 0.8306804299354553 average loss: 0.8561081745242345\n",
      "Epoch 73 loss: 0.6762874722480774 average loss: 0.878874107854057\n",
      "Epoch 74 loss: 0.8102204203605652 average loss: 0.8667413685613006\n",
      "Epoch 75 loss: 0.9231029748916626 average loss: 0.8346078937290279\n",
      "Epoch 76 loss: 0.6636548042297363 average loss: 0.8536841318807529\n",
      "Epoch 77 loss: 0.8914540410041809 average loss: 0.8626724922930011\n",
      "Epoch 78 loss: 0.5242888927459717 average loss: 0.876618915614281\n",
      "Epoch 79 loss: 0.8359795212745667 average loss: 0.8476062999426863\n",
      "Epoch 80 loss: 0.8591520190238953 average loss: 0.8541087321652711\n",
      "Epoch 81 loss: 0.7026147246360779 average loss: 0.837100662120426\n",
      "Epoch 82 loss: 1.1176234483718872 average loss: 0.8404183026033504\n",
      "Epoch 83 loss: 0.7513271570205688 average loss: 0.8441744160106164\n",
      "Epoch 84 loss: 0.6008473038673401 average loss: 0.8729715790912396\n",
      "Epoch 85 loss: 0.7458420991897583 average loss: 0.8204561672137893\n",
      "Epoch 86 loss: 0.9223056435585022 average loss: 0.8696990823017732\n",
      "Epoch 87 loss: 0.9497491121292114 average loss: 0.8303962384016459\n",
      "Epoch 88 loss: 0.7164262533187866 average loss: 0.8071239420020854\n",
      "Epoch 89 loss: 0.6627942323684692 average loss: 0.8215630077223741\n",
      "Epoch 90 loss: 0.7343551516532898 average loss: 0.8211087598145463\n",
      "Epoch 91 loss: 0.6552891731262207 average loss: 0.8435991010593094\n",
      "Epoch 92 loss: 0.6925773620605469 average loss: 0.8253050780933322\n",
      "Epoch 93 loss: 0.7363500595092773 average loss: 0.8548729394683401\n",
      "Epoch 94 loss: 0.9040025472640991 average loss: 0.8129162995414879\n",
      "Epoch 95 loss: 0.6773679256439209 average loss: 0.8039685535521908\n",
      "Epoch 96 loss: 0.9426693916320801 average loss: 0.8336999605175193\n",
      "Epoch 97 loss: 0.7138339281082153 average loss: 0.8579517063748745\n",
      "Epoch 98 loss: 0.6167041063308716 average loss: 0.8001733372229656\n",
      "Epoch 99 loss: 0.6640784740447998 average loss: 0.8052359095966543\n",
      "Epoch 100 loss: 0.9880213737487793 average loss: 0.7831310404621008\n"
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "for epoch in range(epoch):\n",
    "  total_loss = 0.\n",
    "  for input, target in dataset:\n",
    "    pred = u_model(input)\n",
    "    \n",
    "    loss = loss_fn(pred, target)\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  average_loss = total_loss / len(dataset)\n",
    "  print(f\"Epoch {epoch + 1} loss: {loss.item()} average loss: {average_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5683, grad_fn=<MaxBackward0>),\n",
       " tensor(6),\n",
       " tensor([3.4760e-02, 8.8228e-05, 4.7342e-06, 1.1415e-05, 2.1719e-06, 4.5100e-02,\n",
       "         5.6828e-01, 6.5567e-03, 1.7876e-05, 2.2570e-01, 9.8583e-03, 7.0468e-03,\n",
       "         2.0623e-06, 4.3828e-03, 8.2662e-03, 9.4101e-06, 5.8269e-04, 7.1878e-07,\n",
       "         9.8861e-05, 3.9608e-08, 1.7592e-05, 3.3948e-02, 6.0780e-03, 7.9004e-03,\n",
       "         1.7750e-03, 6.2422e-05, 1.0692e-03, 3.6197e-06, 8.6654e-07, 6.6835e-04,\n",
       "         5.5175e-03, 5.5425e-04, 1.2778e-03, 3.9203e-06, 1.8333e-04, 1.5109e-04,\n",
       "         7.6933e-04, 7.0673e-05, 3.6397e-05, 2.8279e-04, 5.4389e-05, 7.2091e-05,\n",
       "         5.5369e-05, 9.1733e-05, 3.5258e-05, 9.3644e-04, 5.4624e-03, 2.0505e-06,\n",
       "         6.7434e-07, 1.6814e-06, 5.2219e-03, 1.3156e-02, 2.6344e-06, 2.4629e-06,\n",
       "         1.7839e-05, 2.4677e-03, 1.2726e-03, 7.3382e-09, 9.4290e-07, 1.2736e-07,\n",
       "         2.2475e-09, 1.6622e-08, 2.6550e-09, 2.5690e-09],\n",
       "        grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_tokens = u_tokenizer.encode(\"the capital of the united states is london. the capital of france is\")\n",
    "new_tokens = new_tokens.detach().cpu().numpy().tolist()\n",
    "new_tokens.append(61)\n",
    "\n",
    "out = u_model(torch.tensor(new_tokens))\n",
    "\n",
    "probs = torch.softmax(out[-1], dim=-1)\n",
    "max_prob, max_index = torch.max(probs, dim=-1)\n",
    "max_prob, max_index, probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
